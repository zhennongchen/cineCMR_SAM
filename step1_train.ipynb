{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "You should prepare the following before running this step. Please refer to the `example_data/data` folder for guidance:\n",
    "\n",
    "1. **SAX data** including image and manual segmentation (for training)\n",
    "   - you want to prepare the SAX data as a 4D array [x,y,time_frame,slice_num] saved as a nii file. in our study we sample 15 time frames as default\n",
    "   - please refer ```example_data/data/ID_0002``` as SAX reference  \n",
    "\n",
    "2. **LAX data** including image and manual segmentation (for training), if you need to train for LAX segmentation as well\n",
    "   - you want to prepare the LAX data as a 3D array [x,y,time_frame]. As aforementioned, time frame is default to 15\n",
    "   - please refer ```example_data/data/ID_0085``` as LAX reference \n",
    "\n",
    "3. **A patient list** that enumerates all your cases\n",
    "   - To understand the standard format, please refer to the file:  \n",
    "     `example_data/Patient_list/patient_list.xlsx`\n",
    "   - make sure column ***total_slice_num*** is correct for each case\n",
    "\n",
    "4. **Text prompts** that specifies the view type\n",
    "   - our model takes text prompt \"SAX\" or \"LAX\" to specify the view type \n",
    "   - we use \"CLIP\" model to embed text prompts (code: ```dataset/CMR/clip_extractor.ipynb```)\n",
    "   - we have prepared the embedded feature in `example_data/data/text_prompt_clip`, please download to your local\n",
    "\n",
    "5. **Box prompts** that indicates the location of myocardium\n",
    "   - during the training, the box prompts are automatically generated based on manual segmentation so don't need to worry about it\n",
    "\n",
    "6. **Original SAM model**\n",
    "   - download from [this link](https://github.com/SekeunKim/MediViSTA?tab=readme-ov-file)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Docker environment\n",
    "Please use `docker`, it will build a pytorch-based container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/Documents')  ### remove this if not needed!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import nibabel as nb\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "from einops import rearrange\n",
    "from natsort import natsorted\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    " \n",
    "from cineCMR_SAM.utils.model_util import *\n",
    "from cineCMR_SAM.segment_anything.model import build_model \n",
    "from cineCMR_SAM.utils.save_utils import *\n",
    "from cineCMR_SAM.utils.config_util import Config\n",
    "from cineCMR_SAM.utils.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "from cineCMR_SAM.train_engine import train_loop\n",
    "\n",
    "import cineCMR_SAM.dataset.build_CMR_datasets as build_CMR_datasets\n",
    "import cineCMR_SAM.functions_collection as ff\n",
    "import cineCMR_SAM.get_args_parser as get_args_parser\n",
    "\n",
    "main_path = '/mnt/camca_NAS/SAM_for_CMR/'  # replace with your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters for this experiment\n",
    "The full setting can be find in ```get_args_parser.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment-specific parameters\n",
    "trial_name = 'cineCMR_sam_trial' \n",
    "\n",
    "output_dir = os.path.join(main_path, 'example_data/models', trial_name)\n",
    "ff.make_folder([os.path.join(main_path, 'example_data/models'), output_dir])\n",
    "\n",
    "text_prompt = True # whether we need to input text prompt to specify the view types (LAX or SAX). True or False. default = True\n",
    "box_prompt = False # whether we have the bounding box for myocardium defined by the user. False means no box, 'one' means one box at ED and 'two' means two boxes at ED and ES\n",
    "\n",
    "# preload the text prompt feature (\n",
    "sax_text_prompt_feature = np.load(os.path.join(main_path,'example_data/data/text_prompt_clip/sax.npy'))\n",
    "lax_text_prompt_feature = np.load(os.path.join(main_path,'example_data/data/text_prompt_clip/lax.npy'))\n",
    "\n",
    "# define the original SAM model\n",
    "original_sam = os.path.join( main_path, 'example_data/pretrained_sam/sam_vit_h_4b8939.pth') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = None # define your pre-trained model if any\n",
    "start_epoch = 1\n",
    "total_training_epochs = 100 # define total number of epochs\n",
    "\n",
    "args = get_args_parser.get_args_parser(text_prompt = text_prompt, \n",
    "                                       box_prompt = box_prompt, \n",
    "                                       pretrained_model = pretrained_model, \n",
    "                                       original_sam = original_sam, \n",
    "                                       start_epoch = start_epoch, \n",
    "                                       total_training_epochs = total_training_epochs)\n",
    "args = args.parse_args([])\n",
    "\n",
    "# some other settings\n",
    "cfg = Config(args.config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the training dataset (from SAX or/and LAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAX training data\n",
    "patient_list_file_sax = os.path.join(main_path,'example_data/data/Patient_list/patient_list_sax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_sax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'sax',\n",
    "        patient_list_file = patient_list_file_sax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = sax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "# define LAX training data\n",
    "patient_list_file_lax = os.path.join(main_path,'example_data/data/Patient_list/patient_list_lax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_lax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'lax',\n",
    "        patient_list_file = patient_list_file_lax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = lax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "dataset_train = [dataset_train_sax, dataset_train_lax]\n",
    "\n",
    "'''Set up data loader for training and validation set'''\n",
    "data_loader_train = []\n",
    "for i in range(len(dataset_train)):\n",
    "    data_loader_train.append(torch.utils.data.DataLoader(dataset_train[i], batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pre-trained SAM model (freeze SAM modules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important! text prompt: True\n",
      "Important! box prompt: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14951/4199788675.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model :  /mnt/camca_NAS/SAM_for_CMR/models/sam_multiview_prompt_text_HF_finetune_on_refine_firstround/models/model-82.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_14951/4199788675.py:40: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  finetune_checkpoint = torch.load(args.pretrained_model)\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "model = build_model(args, device)\n",
    "\n",
    "# set freezed and trainable keys\n",
    "train_keys = []\n",
    "freezed_keys = []\n",
    "        \n",
    "# load pretrained sam model vit_h\n",
    "if args.model_type.startswith(\"sam\"):\n",
    "    if args.resume.endswith(\".pth\"):\n",
    "        with open(args.resume, \"rb\") as f:\n",
    "            state_dict = torch.load(f)\n",
    "        try:\n",
    "            model.load_state_dict(state_dict)\n",
    "        except:\n",
    "            if args.vit_type == \"vit_h\":\n",
    "                new_state_dict = load_from(model, state_dict, args.img_size,  16, [7, 15, 23, 31])\n",
    "               \n",
    "            model.load_state_dict(new_state_dict)\n",
    "        \n",
    "        # freeze original SAM layers\n",
    "        freeze_list = [ \"norm1\", \"attn\" , \"mlp\", \"norm2\"]  \n",
    "                \n",
    "        for n, value in model.named_parameters():\n",
    "            if any(substring in n for substring in freeze_list):\n",
    "                freezed_keys.append(n)\n",
    "                value.requires_grad = False\n",
    "            else:\n",
    "                train_keys.append(n)\n",
    "                value.requires_grad = True\n",
    "\n",
    "## Select optimization method\n",
    "optimizer = MADGRAD(model.parameters(), lr=args.lr) # momentum=,weight_decay=,eps=)\n",
    "        \n",
    "# Continue training model\n",
    "if args.pretrained_model is not None:\n",
    "    if os.path.exists(args.pretrained_model):\n",
    "        print('loading pretrained model : ', args.pretrained_model)\n",
    "        args.resume = args.pretrained_model\n",
    "        finetune_checkpoint = torch.load(args.pretrained_model)\n",
    "        model.load_state_dict(finetune_checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(finetune_checkpoint[\"optimizer\"])\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('new training\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "learning rate now: 9.025e-05\n",
      "in train loop we have turn_zero_seg_slice_into:  10\n",
      "in training current slice type:  sax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Documents/cineCMR_SAM/utils/misc.py:252: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n",
      "/workspace/Documents/cineCMR_SAM/train_engine.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 0/11]  eta: 0:00:33  lr: 0.000090  loss1: 0.0613 (0.0613)  time: 3.0093  data: 2.0194  max mem: 23355\n",
      "Epoch: [1]  [10/11]  eta: 0:00:00  lr: 0.000090  loss1: 0.0553 (0.0561)  time: 0.5526  data: 0.3483  max mem: 23355\n",
      "Epoch: [1] Total time: 0:00:06 (0.5527 s / it)\n",
      "in training current slice type:  lax\n",
      "Epoch: [1]  [0/1]  eta: 0:00:00  lr: 0.000090  loss1: 0.0553 (0.0673)  time: 0.2644  data: 0.1368  max mem: 23355\n",
      "Epoch: [1] Total time: 0:00:00 (0.2653 s / it)\n",
      "in epoch:  1  training average_loss:  0.06731000170111656  average_lossCE:  0.032660245119283594  average_lossDICE:  0.06929951409498851  sax_loss:  0.056121711026538505  sax_lossCE:  0.02998106706548821  sax_lossDICE:  0.052281287583437835  lax_loss:  0.19038119912147522  lax_lossCE:  0.06213120371103287  lax_lossDICE:  0.2565000057220459\n",
      "now run on_epoch_end function\n",
      "now run on_epoch_end function\n"
     ]
    }
   ],
   "source": [
    "training_log = []\n",
    "model_save_folder = os.path.join(output_dir, 'models'); ff.make_folder([output_dir, model_save_folder])\n",
    "log_save_folder = os.path.join(output_dir, 'logs'); ff.make_folder([log_save_folder])\n",
    "\n",
    "for epoch in range(args.start_epoch, args.start_epoch + args.total_training_epochs):\n",
    "        print('training epoch:', epoch)\n",
    "\n",
    "        if epoch % args.lr_update_every_N_epoch == 0:\n",
    "            optimizer.param_groups[0][\"lr\"] = optimizer.param_groups[0][\"lr\"] * args.lr_decay_gamma\n",
    "        print('learning rate now:', optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        loss_scaler = NativeScaler()\n",
    "            \n",
    "        train_results = train_loop(\n",
    "                model = model,\n",
    "                data_loader_train  = data_loader_train,\n",
    "                optimizer = optimizer,\n",
    "                epoch = epoch, \n",
    "                loss_scaler = loss_scaler,\n",
    "                args = args,\n",
    "                inputtype = cfg.data.input_type)   \n",
    "        \n",
    "        loss, lossCE, lossDICE, sax_loss, sax_lossCE, sax_lossDICE, lax_loss, lax_lossCE, lax_lossDICE = train_results       \n",
    "            \n",
    "        print('in epoch: ', epoch, ' training average_loss: ', loss, ' average_lossCE: ', lossCE, ' average_lossDICE: ', lossDICE, ' sax_loss: ', sax_loss, ' sax_lossCE: ', sax_lossCE, ' sax_lossDICE: ', sax_lossDICE, ' lax_loss: ', lax_loss, ' lax_lossCE: ', lax_lossCE, ' lax_lossDICE: ', lax_lossDICE)\n",
    "    \n",
    "        # on_epoch_end:\n",
    "        for k in range(len(dataset_train)):\n",
    "            dataset_train[k].on_epoch_end()\n",
    "    \n",
    "        if  epoch % args.save_model_file_every_N_epoch == 0 or (epoch + 1) == args.start_epoch + args.total_training_epochs:\n",
    "            checkpoint_path = os.path.join(model_save_folder,  'model-%s.pth' % epoch)\n",
    "            to_save = {\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'scaler': loss_scaler.state_dict(),\n",
    "                        'args': args,}\n",
    "            torch.save(to_save, checkpoint_path)\n",
    "\n",
    "        training_log.append([epoch, optimizer.param_groups[0][\"lr\"], train_results[0], train_results[1], train_results[2], train_results[3], train_results[4], train_results[5], train_results[6], train_results[7], train_results[8]])\n",
    "        df = pd.DataFrame(training_log, columns=['epoch', 'lr','average_loss', 'average_lossCE', 'average_lossDICE', 'sax_loss', 'sax_lossCE', 'sax_lossDICE', 'lax_loss', 'lax_lossCE', 'lax_lossDICE'])\n",
    "        df.to_excel(os.path.join(log_save_folder, 'training_log.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
