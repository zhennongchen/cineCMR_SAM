{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/Documents')  ### remove this if not needed!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import nibabel as nb\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "from einops import rearrange\n",
    "from natsort import natsorted\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    " \n",
    "from cineCMR_SAM.utils.model_util import *\n",
    "from cineCMR_SAM.segment_anything.model import build_model \n",
    "from cineCMR_SAM.utils.save_utils import *\n",
    "from cineCMR_SAM.utils.config_util import Config\n",
    "from cineCMR_SAM.utils.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "from cineCMR_SAM.train_engine import train_loop\n",
    "\n",
    "import cineCMR_SAM.dataset.build_CMR_datasets as build_CMR_datasets\n",
    "import cineCMR_SAM.functions_collection as ff\n",
    "import cineCMR_SAM.get_args_parser as get_args_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment-specific parameters\n",
    "main_path = '/mnt/camca_NAS/SAM_for_CMR/'  # replace with your own path\n",
    "trial_name = 'cineCMR_sam_github'\n",
    "output_dir = os.path.join(main_path, 'models', trial_name) # replace with your own path\n",
    "text_prompt = True # whether we need to input text prompt to specify the view types (LAX or SAX). True or False. default = True\n",
    "box_prompt = False # whether we have the bounding box for myocardium defined by the user. None means no box, 'one' means one box at ED and 'two' means two boxes at ED and ES\n",
    "\n",
    "pretrained_model = None # define your pre-trained model if any\n",
    "start_epoch = 1\n",
    "total_training_epochs = 1 # define total number of epochs\n",
    "\n",
    "# preload the text prompt feature (it's the output of a CLIP model when I input \"LAX\" or \"SAX\" into it)\n",
    "sax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/sax.npy')\n",
    "lax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/lax.npy')\n",
    "\n",
    "# also define the original SAM model, replace with your own path (you can easily download the original SAM model from online)\n",
    "original_sam = os.path.join( \"/mnt/camca_NAS/SAM_for_CMR/\", 'models/pretrained_sam/sam_vit_h_4b8939.pth') \n",
    "\n",
    "args = get_args_parser.get_args_parser(text_prompt = text_prompt, box_prompt = box_prompt, pretrained_model = pretrained_model, original_sam = original_sam, start_epoch = start_epoch, total_training_epochs = total_training_epochs)\n",
    "args = args.parse_args([])\n",
    "\n",
    "# some other settings\n",
    "cfg = Config(args.config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the training dataset (from SAX or/and LAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAX training data\n",
    "patient_list_file_sax = os.path.join(main_path,'models/cineCMR_sam_github/patient_list_sax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_sax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'sax',\n",
    "        patient_list_file = patient_list_file_sax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = sax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "# define LAX training data\n",
    "patient_list_file_lax = os.path.join(main_path,'models/cineCMR_sam_github/patient_list_lax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_lax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'lax',\n",
    "        patient_list_file = patient_list_file_lax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = lax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "dataset_train = [dataset_train_sax, dataset_train_lax]\n",
    "\n",
    "'''Set up data loader for training and validation set'''\n",
    "data_loader_train = []\n",
    "for i in range(len(dataset_train)):\n",
    "    data_loader_train.append(torch.utils.data.DataLoader(dataset_train[i], batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pre-trained SAM model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important! text prompt: True\n",
      "Important! box prompt: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3530388/1542652002.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new training\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "model = build_model(args, device)\n",
    "\n",
    "# set freezed and trainable keys\n",
    "train_keys = []\n",
    "freezed_keys = []\n",
    "        \n",
    "# load pretrained sam model vit_h\n",
    "if args.model_type.startswith(\"sam\"):\n",
    "    if args.resume.endswith(\".pth\"):\n",
    "        with open(args.resume, \"rb\") as f:\n",
    "            state_dict = torch.load(f)\n",
    "        try:\n",
    "            model.load_state_dict(state_dict)\n",
    "        except:\n",
    "            if args.vit_type == \"vit_h\":\n",
    "                new_state_dict = load_from(model, state_dict, args.img_size,  16, [7, 15, 23, 31])\n",
    "               \n",
    "            model.load_state_dict(new_state_dict)\n",
    "        \n",
    "        # freeze original SAM layers\n",
    "        freeze_list = [ \"norm1\", \"attn\" , \"mlp\", \"norm2\"]  \n",
    "                \n",
    "        for n, value in model.named_parameters():\n",
    "            if any(substring in n for substring in freeze_list):\n",
    "                freezed_keys.append(n)\n",
    "                value.requires_grad = False\n",
    "            else:\n",
    "                train_keys.append(n)\n",
    "                value.requires_grad = True\n",
    "\n",
    "## Select optimization method\n",
    "optimizer = MADGRAD(model.parameters(), lr=args.lr) # momentum=,weight_decay=,eps=)\n",
    "        \n",
    "# Continue training model\n",
    "if args.pretrained_model is not None:\n",
    "    if os.path.exists(args.pretrained_model):\n",
    "        print('loading pretrained model : ', args.pretrained_model)\n",
    "        args.resume = args.pretrained_model\n",
    "        finetune_checkpoint = torch.load(args.pretrained_model)\n",
    "        model.load_state_dict(finetune_checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(finetune_checkpoint[\"optimizer\"])\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('new training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 1\n",
      "learning rate now: 0.0001\n",
      "in train loop we have turn_zero_seg_slice_into:  10\n",
      "in training current slice type:  sax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Documents/cineCMR_SAM/utils/misc.py:252: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self._scaler = torch.cuda.amp.GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/Documents/cineCMR_SAM/train_engine.py:50: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1]  [ 0/11]  eta: 0:00:24  lr: 0.000100  loss1: 5.8597 (5.8597)  time: 2.2047  data: 1.1594  max mem: 18854\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "Epoch: [1]  [10/11]  eta: 0:00:00  lr: 0.000100  loss1: 5.0624 (4.6847)  time: 0.5433  data: 0.3333  max mem: 18854\n",
      "Epoch: [1] Total time: 0:00:05 (0.5436 s / it)\n",
      "in training current slice type:  lax\n",
      "in dataset_SAX, patient_id is:  ID_0085\n",
      "Epoch: [1]  [0/1]  eta: 0:00:00  lr: 0.000100  loss1: 4.9009 (4.6050)  time: 0.6595  data: 0.5343  max mem: 18854\n",
      "Epoch: [1] Total time: 0:00:00 (0.6605 s / it)\n",
      "in epoch:  1  training average_loss:  4.605008602142334  average_lossCE:  4.176875988642375  average_lossDICE:  0.8562652071317037  sax_loss:  4.684697172858498  sax_lossCE:  4.2605744708668105  sax_lossDICE:  0.8482453877275641  lax_loss:  3.7284343242645264  lax_lossCE:  3.256192684173584  lax_lossDICE:  0.94448322057724\n",
      "now run on_epoch_end function\n",
      "now run on_epoch_end function\n"
     ]
    }
   ],
   "source": [
    "training_log = []\n",
    "model_save_folder = os.path.join(output_dir, 'models'); ff.make_folder([output_dir, model_save_folder])\n",
    "log_save_folder = os.path.join(output_dir, 'logs'); ff.make_folder([log_save_folder])\n",
    "\n",
    "for epoch in range(args.start_epoch, args.start_epoch + args.total_training_epochs):\n",
    "        print('training epoch:', epoch)\n",
    "\n",
    "        if epoch % args.lr_update_every_N_epoch == 0:\n",
    "            optimizer.param_groups[0][\"lr\"] = optimizer.param_groups[0][\"lr\"] * args.lr_decay_gamma\n",
    "        print('learning rate now:', optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        loss_scaler = NativeScaler()\n",
    "            \n",
    "        train_results = train_loop(\n",
    "                model = model,\n",
    "                data_loader_train  = data_loader_train,\n",
    "                optimizer = optimizer,\n",
    "                epoch = epoch, \n",
    "                loss_scaler = loss_scaler,\n",
    "                args = args,\n",
    "                inputtype = cfg.data.input_type)   \n",
    "        \n",
    "        loss, lossCE, lossDICE, sax_loss, sax_lossCE, sax_lossDICE, lax_loss, lax_lossCE, lax_lossDICE = train_results       \n",
    "            \n",
    "        print('in epoch: ', epoch, ' training average_loss: ', loss, ' average_lossCE: ', lossCE, ' average_lossDICE: ', lossDICE, ' sax_loss: ', sax_loss, ' sax_lossCE: ', sax_lossCE, ' sax_lossDICE: ', sax_lossDICE, ' lax_loss: ', lax_loss, ' lax_lossCE: ', lax_lossCE, ' lax_lossDICE: ', lax_lossDICE)\n",
    "    \n",
    "        # on_epoch_end:\n",
    "        for k in range(len(dataset_train)):\n",
    "            dataset_train[k].on_epoch_end()\n",
    "    \n",
    "        if  epoch % args.save_model_file_every_N_epoch == 0 or (epoch + 1) == args.start_epoch + args.total_training_epochs:\n",
    "            checkpoint_path = os.path.join(model_save_folder,  'model-%s.pth' % epoch)\n",
    "            to_save = {\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'scaler': loss_scaler.state_dict(),\n",
    "                        'args': args,}\n",
    "            torch.save(to_save, checkpoint_path)\n",
    "\n",
    "        training_log.append([epoch, optimizer.param_groups[0][\"lr\"], train_results[0], train_results[1], train_results[2], train_results[3], train_results[4], train_results[5], train_results[6], train_results[7], train_results[8]])\n",
    "        df = pd.DataFrame(training_log, columns=['epoch', 'lr','average_loss', 'average_lossCE', 'average_lossDICE', 'sax_loss', 'sax_lossCE', 'sax_lossDICE', 'lax_loss', 'lax_lossCE', 'lax_lossDICE'])\n",
    "        df.to_excel(os.path.join(log_save_folder, 'training_log.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
