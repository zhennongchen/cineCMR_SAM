{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/Documents')  ### remove this if not needed!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "import nibabel as nb\n",
    "import time\n",
    "\n",
    "import argparse\n",
    "from einops import rearrange\n",
    "from natsort import natsorted\n",
    "from madgrad import MADGRAD\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    " \n",
    "from cineCMR_SAM.utils.model_util import *\n",
    "from cineCMR_SAM.segment_anything.model import build_model \n",
    "from cineCMR_SAM.utils.save_utils import *\n",
    "from cineCMR_SAM.utils.config_util import Config\n",
    "from cineCMR_SAM.utils.misc import NativeScalerWithGradNormCount as NativeScaler\n",
    "\n",
    "from cineCMR_SAM.train_engine import train_loop\n",
    "\n",
    "import cineCMR_SAM.dataset.build_CMR_datasets as build_CMR_datasets\n",
    "import cineCMR_SAM.functions_collection as ff\n",
    "import cineCMR_SAM.get_args_parser as get_args_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment-specific parameters\n",
    "main_path = '/mnt/camca_NAS/SAM_for_CMR/'  # replace with your own path\n",
    "trial_name = 'cineCMR_sam_github'\n",
    "output_dir = os.path.join(main_path, 'models', trial_name) # replace with your own path\n",
    "text_prompt = True # whether we need to input text prompt to specify the view types (LAX or SAX). True or False. default = True\n",
    "box_prompt = False # whether we have the bounding box for myocardium defined by the user. None means no box, 'one' means one box at ED and 'two' means two boxes at ED and ES\n",
    "\n",
    "pretrained_model = None\n",
    "start_epoch = 1\n",
    "total_training_epochs = 1\n",
    "\n",
    "# preload the text prompt feature (it's the output of a CLIP model when I input \"LAX\" or \"SAX\" into it)\n",
    "sax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/sax.npy')\n",
    "lax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/lax.npy')\n",
    "\n",
    "# also define the original SAM model\n",
    "original_sam = os.path.join( \"/mnt/camca_NAS/SAM_for_CMR/\", 'models/pretrained_sam/sam_vit_h_4b8939.pth') # replace with your own path (you can easily download the original SAM model from online)\n",
    "\n",
    "args = get_args_parser.get_args_parser(text_prompt = text_prompt, box_prompt = box_prompt, pretrained_model = pretrained_model, original_sam = original_sam, start_epoch = start_epoch, total_training_epochs = total_training_epochs)\n",
    "args = args.parse_args([])\n",
    "\n",
    "# some other settings\n",
    "cfg = Config(args.config)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the training dataset (from SAX or/and LAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define SAX training data\n",
    "patient_list_file_sax = os.path.join(main_path,'models/cineCMR_sam_github/patient_list_sax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_sax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'sax',\n",
    "        patient_list_file = patient_list_file_sax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = sax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "# define LAX training data\n",
    "patient_list_file_lax = os.path.join(main_path,'models/cineCMR_sam_github/patient_list_lax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "dataset_train_lax = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = 'lax',\n",
    "        patient_list_file = patient_list_file_lax, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = lax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = True, \n",
    "        augment = True)\n",
    "\n",
    "dataset_train = [dataset_train_sax, dataset_train_lax]\n",
    "\n",
    "'''Set up data loader for training and validation set'''\n",
    "data_loader_train = []\n",
    "for i in range(len(dataset_train)):\n",
    "    data_loader_train.append(torch.utils.data.DataLoader(dataset_train[i], batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load pre-trained SAM model and set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important! text prompt: True\n",
      "Important! box prompt: False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2885231/1542652002.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict = torch.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model :  /mnt/camca_NAS/SAM_for_CMR/models/cineCMR_sam_github/models/model_80.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2885231/1542652002.py:39: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  finetune_checkpoint = torch.load(args.pretrained_model)\n"
     ]
    }
   ],
   "source": [
    "# set model\n",
    "model = build_model(args, device)\n",
    "\n",
    "# set freezed and trainable keys\n",
    "train_keys = []\n",
    "freezed_keys = []\n",
    "        \n",
    "# load pretrained sam model vit_h\n",
    "if args.model_type.startswith(\"sam\"):\n",
    "    if args.resume.endswith(\".pth\"):\n",
    "        with open(args.resume, \"rb\") as f:\n",
    "            state_dict = torch.load(f)\n",
    "        try:\n",
    "            model.load_state_dict(state_dict)\n",
    "        except:\n",
    "            if args.vit_type == \"vit_h\":\n",
    "                new_state_dict = load_from(model, state_dict, args.img_size,  16, [7, 15, 23, 31])\n",
    "               \n",
    "            model.load_state_dict(new_state_dict)\n",
    "                   \n",
    "        freeze_list = [ \"norm1\", \"attn\" , \"mlp\", \"norm2\"]  \n",
    "                \n",
    "        for n, value in model.named_parameters():\n",
    "            if any(substring in n for substring in freeze_list):\n",
    "                freezed_keys.append(n)\n",
    "                value.requires_grad = False\n",
    "            else:\n",
    "                train_keys.append(n)\n",
    "                value.requires_grad = True\n",
    "\n",
    "## Select optimization method\n",
    "optimizer = MADGRAD(model.parameters(), lr=args.lr) # momentum=,weight_decay=,eps=)\n",
    "        \n",
    "# Continue training model\n",
    "if args.pretrained_model is not None:\n",
    "    if os.path.exists(args.pretrained_model):\n",
    "        print('loading pretrained model : ', args.pretrained_model)\n",
    "        args.resume = args.pretrained_model\n",
    "        finetune_checkpoint = torch.load(args.pretrained_model)\n",
    "        model.load_state_dict(finetune_checkpoint[\"model\"])\n",
    "        optimizer.load_state_dict(finetune_checkpoint[\"optimizer\"])\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    print('new training\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training epoch: 81\n",
      "learning rate now: 9.025e-05\n",
      "in train loop we have turn_zero_seg_slice_into:  10\n",
      "in training current slice type:  sax\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "Epoch: [81]  [ 0/11]  eta: 0:00:10  lr: 0.000090  loss1: 0.0912 (0.0912)  time: 0.9675  data: 0.7135  max mem: 18854\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "Epoch: [81]  [10/11]  eta: 0:00:00  lr: 0.000090  loss1: 0.1227 (0.1182)  time: 0.5965  data: 0.3273  max mem: 18854\n",
      "Epoch: [81] Total time: 0:00:06 (0.5967 s / it)\n",
      "in training current slice type:  lax\n",
      "in dataset_SAX, patient_id is:  ID_0085\n",
      "Epoch: [81]  [0/1]  eta: 0:00:00  lr: 0.000090  loss1: 0.1227 (0.1261)  time: 0.9758  data: 0.6966  max mem: 18854\n",
      "Epoch: [81] Total time: 0:00:00 (0.9767 s / it)\n",
      "in epoch:  81  training average_loss:  0.12610399536788464  average_lossCE:  0.06175902392715216  average_lossDICE:  0.12868993977705637  sax_loss:  0.11816657876426523  sax_lossCE:  0.06003339757973498  sax_lossDICE:  0.11626635898243297  lax_loss:  0.21341557800769806  lax_lossCE:  0.08074091374874115  lax_lossDICE:  0.2653493285179138\n",
      "now run on_epoch_end function\n",
      "now run on_epoch_end function\n"
     ]
    }
   ],
   "source": [
    "training_log = []\n",
    "model_save_folder = os.path.join(output_dir, 'models'); ff.make_folder([output_dir, model_save_folder])\n",
    "log_save_folder = os.path.join(output_dir, 'logs'); ff.make_folder([log_save_folder])\n",
    "\n",
    "for epoch in range(args.start_epoch, args.start_epoch + args.total_training_epochs):\n",
    "        print('training epoch:', epoch)\n",
    "\n",
    "        if epoch % args.lr_update_every_N_epoch == 0:\n",
    "            optimizer.param_groups[0][\"lr\"] = optimizer.param_groups[0][\"lr\"] * args.lr_decay_gamma\n",
    "        print('learning rate now:', optimizer.param_groups[0][\"lr\"])\n",
    "        \n",
    "        loss_scaler = NativeScaler()\n",
    "            \n",
    "        train_results = train_loop(\n",
    "                model = model,\n",
    "                data_loader_train  = data_loader_train,\n",
    "                optimizer = optimizer,\n",
    "                epoch = epoch, \n",
    "                loss_scaler = loss_scaler,\n",
    "                args = args,\n",
    "                inputtype = cfg.data.input_type)   \n",
    "        \n",
    "        loss, lossCE, lossDICE, sax_loss, sax_lossCE, sax_lossDICE, lax_loss, lax_lossCE, lax_lossDICE = train_results       \n",
    "            \n",
    "        print('in epoch: ', epoch, ' training average_loss: ', loss, ' average_lossCE: ', lossCE, ' average_lossDICE: ', lossDICE, ' sax_loss: ', sax_loss, ' sax_lossCE: ', sax_lossCE, ' sax_lossDICE: ', sax_lossDICE, ' lax_loss: ', lax_loss, ' lax_lossCE: ', lax_lossCE, ' lax_lossDICE: ', lax_lossDICE)\n",
    "    \n",
    "        # on_epoch_end:\n",
    "        for k in range(len(dataset_train)):\n",
    "            dataset_train[k].on_epoch_end()\n",
    "    \n",
    "        if  epoch % args.save_model_file_every_N_epoch == 0 or (epoch + 1) == args.start_epoch + args.total_training_epochs:\n",
    "            checkpoint_path = os.path.join(model_save_folder,  'model-%s.pth' % epoch)\n",
    "            to_save = {\n",
    "                        'model': model.state_dict(),\n",
    "                        'optimizer': optimizer.state_dict(),\n",
    "                        'epoch': epoch,\n",
    "                        'scaler': loss_scaler.state_dict(),\n",
    "                        'args': args,}\n",
    "            torch.save(to_save, checkpoint_path)\n",
    "\n",
    "        training_log.append([epoch, optimizer.param_groups[0][\"lr\"], train_results[0], train_results[1], train_results[2], train_results[3], train_results[4], train_results[5], train_results[6], train_results[7], train_results[8]])\n",
    "        df = pd.DataFrame(training_log, columns=['epoch', 'lr','average_loss', 'average_lossCE', 'average_lossDICE', 'sax_loss', 'sax_lossCE', 'sax_lossDICE', 'lax_loss', 'lax_lossCE', 'lax_lossDICE'])\n",
    "        df.to_excel(os.path.join(log_save_folder, 'training_log.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
