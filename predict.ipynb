{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/workspace/Documents')  ### remove this if not needed!\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.backends.cudnn as cudnn\n",
    " \n",
    "from cineCMR_SAM.utils.model_util import *\n",
    "from cineCMR_SAM.segment_anything.model import build_model \n",
    "from cineCMR_SAM.utils.save_utils import *\n",
    "from cineCMR_SAM.utils.config_util import Config\n",
    "\n",
    "import cineCMR_SAM.inference_engine as inference_engine\n",
    "\n",
    "import cineCMR_SAM.dataset.build_CMR_datasets as build_CMR_datasets\n",
    "import cineCMR_SAM.functions_collection as ff\n",
    "import cineCMR_SAM.get_args_parser as get_args_parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define parameters for this experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set experiment-specific parameters\n",
    "main_path = '/mnt/camca_NAS/SAM_for_CMR/'  # replace with your own path\n",
    "trial_name = 'cineCMR_sam_github'\n",
    "text_prompt = True # whether we need to input text prompt to specify the view types (LAX or SAX). True or False. default = True\n",
    "box_prompt = False # whether we have the bounding box for myocardium defined by the user. None means no box, 'one' means one box at ED and 'two' means two boxes at ED and ES\n",
    "\n",
    "if box_prompt == 'two':\n",
    "    pretrained_model = os.path.join(main_path, 'models',trial_name, 'models/model_text_2boxes.pth')  # replace with your own model\n",
    "elif box_prompt == 'one':\n",
    "    pretrained_model = os.path.join(main_path, 'models',trial_name, 'models/model_text_1box.pth')  # replace with your own model\n",
    "else:\n",
    "    pretrained_model = os.path.join(main_path, 'models',trial_name, 'models/model_text.pth') # replace with your own model\n",
    "\n",
    "# preload the text prompt feature (it's the output of a CLIP model when I input \"LAX\" or \"SAX\" into it)\n",
    "sax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/sax.npy')\n",
    "lax_text_prompt_feature = np.load('/mnt/camca_NAS/SAM_for_CMR/data/text_prompt_clip/lax.npy')\n",
    "\n",
    "# also define the original SAM model \n",
    "original_sam = os.path.join( \"/mnt/camca_NAS/SAM_for_CMR/\", 'models/pretrained_sam/sam_vit_h_4b8939.pth') # replace with your own path (you can easily download the original SAM model from online)\n",
    "\n",
    "args = get_args_parser.get_args_parser(text_prompt = text_prompt, box_prompt = box_prompt, pretrained_model = pretrained_model, original_sam = original_sam)\n",
    "args = args.parse_args([])\n",
    "\n",
    "# some other settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define our data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sax_or_lax = 'sax'\n",
    "save_folder_name = 'predicts_'+sax_or_lax\n",
    "patient_list_file = os.path.join(main_path,'models/cineCMR_sam_github/patient_list_sax.xlsx') if sax_or_lax == 'sax' else os.path.join(main_path,'models/cineCMR_sam_github/patient_list_lax.xlsx')\n",
    "patient_index_list = np.arange(0,1,1)\n",
    "\n",
    "dataset_pred = build_CMR_datasets.build_dataset(\n",
    "        args,\n",
    "        view_type = sax_or_lax,\n",
    "        patient_list_file = patient_list_file, \n",
    "        index_list = patient_index_list, \n",
    "        text_prompt_feature = sax_text_prompt_feature if sax_or_lax == 'sax' else lax_text_prompt_feature,\n",
    "        only_myo = True, \n",
    "        shuffle = False, \n",
    "        augment = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3527174/3950759383.py:4: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Important! text prompt: True\n",
      "Important! box prompt: False\n",
      "loading pretrained model :  /mnt/camca_NAS/SAM_for_CMR/models/cineCMR_sam_github/models/model_text.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3527174/3950759383.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  finetune_checkpoint = torch.load(args.pretrained_model)\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:02,  2.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:02,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:03,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:04,  1.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:04,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:05,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:05,  1.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [00:06,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [00:06,  1.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [00:07,  1.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in dataset_SAX, patient_id is:  ID_0002\n",
      "patient_id:  ID_0002  slice_index:  10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [00:08,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "data_loader_pred = torch.utils.data.DataLoader(dataset_pred, batch_size = 1, shuffle = False, pin_memory = True, num_workers = 0)# cpu_count())\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        model = build_model(args, device)#skip_nameing = True, chunk = np.shape(np.zeros(0)))\n",
    "\n",
    "        # load the pretrained model\n",
    "        if args.pretrained_model is not None:\n",
    "            print('loading pretrained model : ', args.pretrained_model)\n",
    "            finetune_checkpoint = torch.load(args.pretrained_model)\n",
    "            model.load_state_dict(finetune_checkpoint[\"model\"])\n",
    "                            \n",
    "        # do the prediction for each slice (2D+T) one by one\n",
    "        for data_iter_step, batch in tqdm(enumerate(data_loader_pred)):\n",
    "                \n",
    "            patient_id = batch[\"patient_id\"][0]\n",
    "            slice_index = batch[\"slice_index\"].item()\n",
    "            print('patient_id: ', patient_id, ' slice_index: ', slice_index)\n",
    "                \n",
    "            save_folder_patient = os.path.join(main_path, 'models',trial_name, save_folder_name, patient_id)\n",
    "            ff.make_folder([os.path.dirname(save_folder_patient), save_folder_patient])\n",
    "\n",
    "            batch[\"image\"]= batch[\"image\"].cuda()\n",
    "\n",
    "            batch[\"text_prompt_feature\"] = batch[\"text_prompt_feature\"].to(torch.float32)\n",
    "\n",
    "            bbox = batch[\"box_prompt\"].detach().cpu().numpy()[0]\n",
    "                    \n",
    "            output = model(batch, args.img_size)\n",
    "\n",
    "            torch.cuda.synchronize()\n",
    "            \n",
    "            inference_engine.save_predictions(view_type = sax_or_lax, batch = batch, output = output, args = args, save_folder_patient = save_folder_patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
